{
    "bugid": "1043069",
    "cveid": [
        "CVE-2013-6458"
    ],
    "summary": "CVE-2013-6458 libvirtd crashes when swapping disks in qemu guest multiple times - qemuMonitorJSONGetBlockStatsInfo segfault [rhel-6.6]",
    "alias": "None",
    "product": "Red Hat Enterprise Linux 6",
    "hardware": "x86_64",
    "os": "Linux",
    "url": "",
    "reported_date": "2013-12-13 21:49 UTC byAlexandre M",
    "attachment": [
        "https://bugzilla-attachments.redhat.com/attachment.cgi?id=837179",
        "https://bugzilla-attachments.redhat.com/attachment.cgi?id=837180",
        "https://bugzilla-attachments.redhat.com/attachment.cgi?id=837181",
        "https://bugzilla-attachments.redhat.com/attachment.cgi?id=837182",
        "https://bugzilla-attachments.redhat.com/attachment.cgi?id=837398"
    ],
    "comment": [
        "Createdattachment 836510[details]contains 4 files\n\nDescription of problem:\n\nWe are facing a common issue here where libvirtd constantly crashes after attaching and detaching multiple times a disk on a qemu guest (windows 7 or linux).\n\nThe problem arises in our production OpenStack cluster. We have scripts in place to do continuous tests for EBS attachments of running instances.\n\nThe problem seems to happen qemuDomainBlockStats() of qemu/qemu_driver.c where the disk parameter (dev_name) to qemuMonitorJSONGetBlockStatsInfo becomes NULL.\n\nI will let you take a deeper look at the segfault information for analysis.\n\nI have attached 2 separate GDB log session with backtraces after the segfault. One is for a windows guest and the other for linux.\n\nI have also attached the 2 qemu logs of both VMs.\n\nVersion-Release number of selected component (if applicable):\n\n# cat /etc/redhat-release\nCentOS release 6.4 (Final)\n# uname -a\nLinux msr-ostck-cmp39.xxx.xxx 2.6.32-358.6.2.el6.x86_64 #1 SMP Thu May 16 20:59:36 UTC 2013 x86_64 x86_64 x86_64 GNU/Linu\n\n# rpm -qa | grep libvirt\nlibvirt-0.10.2-29.el6.1.x86_64\nlibvirt-debuginfo-0.10.2-29.el6.1.x86_64\nlibvirt-client-0.10.2-29.el6.1.x86_64\nlibvirt-python-0.10.2-29.el6.1.x86_64\nlibvirt-devel-0.10.2-29.el6.1.x86_64\n\n\nHow reproducible:\n\nAttach and detach in a loop a EBS disk on an openstack instance. Eventually libvirtd will crash with a segfault on the disk operation.\n\n\nQEMU DOMAIN INFO:\n\n<domain type='kvm' id='3'>\n  <name>instance-msr-0002eb77</name>\n  <uuid>40ad52d1-22be-4359-8bfd-5cb3ea3c9b23</uuid>\n  <memory unit='KiB'>2097152</memory>\n  <currentMemory unit='KiB'>2097152</currentMemory>\n  <vcpu placement='static'>1</vcpu>\n  <sysinfo type='smbios'>\n    <system>\n      <entry name='manufacturer'>Red Hat Inc.</entry>\n      <entry name='product'>OpenStack Nova</entry>\n      <entry name='version'>2013.1.2-4.el6</entry>\n      <entry name='serial'>c14b272b-34a4-ea25-1e75-5f60847ca93b</entry>\n      <entry name='uuid'>40ad52d1-22be-4359-8bfd-5cb3ea3c9b23</entry>\n    </system>\n  </sysinfo>\n  <os>\n    <type arch='x86_64' machine='rhel6.4.0'>hvm</type>\n    <boot dev='hd'/>\n    <smbios mode='sysinfo'/>\n  </os>\n  <features>\n    <acpi/>\n    <apic/>\n  </features>\n  <cpu mode='host-model'>\n    <model fallback='allow'/>\n    <topology sockets='1' cores='1' threads='1'/>\n  </cpu>\n  <clock offset='utc'>\n    <timer name='pit' tickpolicy='delay'/>\n    <timer name='rtc' tickpolicy='catchup'/>\n  </clock>\n  <on_poweroff>destroy</on_poweroff>\n  <on_reboot>restart</on_reboot>\n  <on_crash>destroy</on_crash>\n  <devices>\n    <emulator>/usr/libexec/qemu-kvm</emulator>\n    <disk type='file' device='disk'>\n      <driver name='qemu' type='qcow2' cache='none'/>\n      <source file='/var/lib/nova/instances/40ad52d1-22be-4359-8bfd-5cb3ea3c9b23/disk'/>\n      <target dev='vda' bus='virtio'/>\n      <alias name='virtio-disk0'/>\n      <address type='pci' domain='0x0000' bus='0x00' slot='0x04' function='0x0'/>\n    </disk>\n    <disk type='block' device='disk'>\n      <driver name='qemu' type='raw' cache='none'/>\n      <source dev='/dev/disk/by-path/ip-192.168.0.11:3260-iscsi-iqn.2010-10.org.openstack:volume-8d2304a2-a67c-477e-9c26-9c4587a8bebf-lun-1'/>\n      <target dev='vdxdgapd' bus='virtio'/>\n      <serial>8d2304a2-a67c-477e-9c26-9c4587a8bebf</serial>\n      <alias name='virtio-disk287105055'/>\n      <address type='pci' domain='0x0000' bus='0x00' slot='0x18' function='0x0'/>\n    </disk>\n    <controller type='usb' index='0'>\n      <alias name='usb0'/>\n      <address type='pci' domain='0x0000' bus='0x00' slot='0x01' function='0x2'/>\n    </controller>\n    <interface type='bridge'>\n      <mac address='fa:16:3e:05:78:6d'/>\n      <source bridge='br100'/>\n      <target dev='vnet0'/>\n      <model type='virtio'/>\n      <filterref filter='nova-instance-instance-msr-0002eb77-fa163e05786d'/>\n      <alias name='net0'/>\n      <address type='pci' domain='0x0000' bus='0x00' slot='0x03' function='0x0'/>\n    </interface>\n    <serial type='file'>\n      <source path='/var/lib/nova/instances/40ad52d1-22be-4359-8bfd-5cb3ea3c9b23/console.log'/>\n      <target port='0'/>\n      <alias name='serial0'/>\n    </serial>\n    <serial type='pty'>\n      <source path='/dev/pts/1'/>\n      <target port='1'/>\n      <alias name='serial1'/>\n    </serial>\n    <console type='file'>\n      <source path='/var/lib/nova/instances/40ad52d1-22be-4359-8bfd-5cb3ea3c9b23/console.log'/>\n      <target type='serial' port='0'/>\n      <alias name='serial0'/>\n    </console>\n    <input type='tablet' bus='usb'>\n      <alias name='input0'/>\n    </input>\n    <input type='mouse' bus='ps2'/>\n    <graphics type='vnc' port='5900' autoport='yes' listen='0.0.0.0' keymap='en-us'>\n      <listen type='address' address='0.0.0.0'/>\n    </graphics>\n    <video>\n      <model type='cirrus' vram='9216' heads='1'/>\n      <alias name='video0'/>\n      <address type='pci' domain='0x0000' bus='0x00' slot='0x02' function='0x0'/>\n    </video>\n    <memballoon model='virtio'>\n      <alias name='balloon0'/>\n      <address type='pci' domain='0x0000' bus='0x00' slot='0x05' function='0x0'/>\n    </memballoon>\n  </devices>\n  <seclabel type='dynamic' model='selinux' relabel='yes'>\n    <label>unconfined_u:system_r:svirt_t:s0:c650,c921</label>\n    <imagelabel>unconfined_u:object_r:svirt_image_t:s0:c650,c921</imagelabel>\n  </seclabel>\n</domain>",
        "Createdattachment 837179[details]linux_qemu.log",
        "Createdattachment 837180[details]linux_vm_gdb.txt",
        "Createdattachment 837181[details]win7_mv_gdb.txt",
        "Createdattachment 837182[details]win7_qemu.log",
        "Since you can reproduce this bug, could you turn on debug logs for libvirtd (see for detailshttp://wiki.libvirt.org/page/DebugLogs), try again, and attach the logs?",
        "Hi Jiri,\nThe log level was already at debug with this line in libvirtd.conf:\n\nlog_level = 1\n\nHowever, the log_filters was left with default values.\n\nIs there anything missing? If the filters make a difference, I will change the options and launch the test again.",
        "Well, the logs are missing :-) We need /var/log/libvirt/libvirtd.log (or wherever you told libvirt to store debug logs), the qemu logs only contain error output of qemu process and libvirt logs generated between fork and exec).",
        "You are totally correct! Missed that one ;)\n\nThe problem is, it's very big. We had an automatic loop test which ran for a few hours, so the log file has become extremely long. Fortunately, we had a dedicated machine for the test and only one VM was running with libvirtd.\n\nI'm trying to slim it down to keep only the meaningful information when the test started, but it's still around 4GB at the moment.\n\nWhere should I put this file?",
        "Createdattachment 837398[details]libvirtd.log file\n\nContains debug logs of libvirtd with segfault just before crash",
        "Comment onattachment 837398[details]libvirtd.log file\n\nI extracted just the important messages before crash.",
        "The log file covers too small period of time. Could you try removing all lines that contain virEvent.* from the huge debug log file and xz compress it? I believe the result should be pretty small even if the original log file was huge.",
        "Hi Jiri,\nyou can find the cleansed logs at:https://gist.github.com/alexandrem/8013892/downloadI removed all event lines starting with virtEvent*\n\nThe file weights 50MB, but 845MB when uncompressed.\n\nThis should cover most of the event logs during that day when we launched the test.",
        "Perfect, thanks for the logs. I found the bug and I'm working on a patch.",
        "This bug needs a CVE assigned.  A read-only client should not be able to crash libvirtd.",
        "This is now fixed upstream by v1.2.0-232-gdb86da5:\n\ncommit db86da5ca2109e4006c286a09b6c75bfe10676ad\nAuthor: Jiri Denemark <jdenemar>\nDate:   Thu Dec 19 22:10:04 2013 +0100\n\n    qemu: Do not access stale data in virDomainBlockStatsCVE-2013-6458https://bugzilla.redhat.com/show_bug.cgi?id=1043069When virDomainDetachDeviceFlags is called concurrently to\n    virDomainBlockStats: libvirtd may crash because qemuDomainBlockStats\n    finds a disk in vm->def before getting a job on a domain and uses the\n    disk pointer after getting the job. However, the domain in unlocked\n    while waiting on a job condition and thus data behind the disk pointer\n    may disappear. This happens when thread 1 runs\n    virDomainDetachDeviceFlags and enters monitor to actually remove the\n    disk. Then another thread starts running virDomainBlockStats, finds the\n    disk in vm->def, and while it's waiting on the job condition (owned by\n    the first thread), the first thread finishes the disk removal. When the\n    second thread gets the job, the memory pointed to be the disk pointer is\n    already gone.\n    \n    That said, every API that is going to begin a job should do that before\n    fetching data from vm->def.\n\nI found similar patterns in several other APIs and fixed them by the following commits: v1.2.0-233-gb799259, v1.2.0-234-gf93d2ca, v1.2.0-235-gff5f30b, v1.2.0-236-g3b56425.",
        "Of the five similar patches, virDomainBlockStats, virDomainGetBlockInfo, qemuDomainBlockJobImpl, and virDomainGetBlockIoTune were all present in upstream 0.10.2.  qemuDomainBlockCopy is not a vulnerability like the other four (since it is the only one of the five that can't be executed on a read-only connection), and while it is not in upstream 0.10.2, it was backported into RHEL 6.3.",
        "Hi,\njust wanted to let you know that we've run our attachment test script several times with the latest libvirtd trunk version and this fix has indeed corrected the crash we had been encountering.\n\nThanks a lot.",
        "(In reply to Jiri Denemark fromcomment #15)> This is now fixed upstream by v1.2.0-232-gdb86da5:\n> \n> commit db86da5ca2109e4006c286a09b6c75bfe10676ad\n> Author: Jiri Denemark <jdenemar>\n> Date:   Thu Dec 19 22:10:04 2013 +0100\n> \n>     qemu: Do not access stale data in virDomainBlockStats\n>     \n>CVE-2013-6458>https://bugzilla.redhat.com/show_bug.cgi?id=1043069>     \n>     When virDomainDetachDeviceFlags is called concurrently to\n>     virDomainBlockStats: libvirtd may crash because qemuDomainBlockStats\n>     finds a disk in vm->def before getting a job on a domain and uses the\n>     disk pointer after getting the job. However, the domain in unlocked\n>     while waiting on a job condition and thus data behind the disk pointer\n>     may disappear. This happens when thread 1 runs\n>     virDomainDetachDeviceFlags and enters monitor to actually remove the\n>     disk. Then another thread starts running virDomainBlockStats, finds the\n>     disk in vm->def, and while it's waiting on the job condition (owned by\n>     the first thread), the first thread finishes the disk removal. When the\n>     second thread gets the job, the memory pointed to be the disk pointer is\n>     already gone.\n>     \n>     That said, every API that is going to begin a job should do that before\n>     fetching data from vm->def.\n> \n> I found similar patterns in several other APIs and fixed them by the\n> following commits: v1.2.0-233-gb799259, v1.2.0-234-gf93d2ca,\n> v1.2.0-235-gff5f30b, v1.2.0-236-g3b56425.I tried to reproduce it this month, but could not. I used virsh detach-device with --config and virsh domblkstat in two console at the same time repeatedly. \n\nIs there any method by using combined virsh commands to reproduce it?\n\nThanks.",
        "(In reply to Hu Jianwei fromcomment #19)> I tried to reproduce it this month, but could not. I used virsh\n> detach-device with --config and virsh domblkstat in two console at the same\n> time repeatedly.I wonder where you got the --config from but it's wrong. You need to hot-unplug the device from a running domain so either no option or --live (but definitely not --config). Anyway, you can checkbug 1054804for a python reproducer which seems to hit the bug more reliably.",
        "(In reply to Jiri Denemark fromcomment #20)> (In reply to Hu Jianwei fromcomment #19)\n> > I tried to reproduce it this month, but could not. I used virsh\n> > detach-device with --config and virsh domblkstat in two console at the same\n> > time repeatedly. \n> \n> I wonder where you got the --config from but it's wrong. You need to\n> hot-unplug the device from a running domain so either no option or --live\n> (but definitely not --config). Anyway, you can checkbug 1054804for a\n> python reproducer which seems to hit the bug more reliably.Furthermore, the bug is only observable if one thread is hotplugging/hot-unplugging disks while the other thread attempts one of the vulnerable APIs.  In other words, to fully reproduce it, you need one thread that is repeatedly cycling a disk in and back out of a running guest, while the other thread is repeatedly trying the problematic API.  The problem that the patches fixed is a use-after-free, so running under valgrind may show the bug more reliably than waiting for a libvirtd crash; or it may slow things down to the point that you don't hit the race as frequently.  The only reliable way to reproduce the problem is to insert strategic sleep() into the hotplug code and recompile.",
        "Thanks for Jiri and Eric comments, it's easy to reproduce it with below two attached python scripts on libvirt-0.10.2-29.el6.1.x86_64 version.",
        "Createdattachment 862098[details]Python scripts for reproducing bug",
        "I can reproduce it on libvirt-0.10.2-29.el6, can not reproduce it on below version:\n\nlibvirt-0.10.2-31.el6.x86_64\nkernel-2.6.32-456.el6.x86_64\nqemu-kvm-rhev-0.12.1.2-2.423.el6.x86_64\n\nSteps:\n1.Download python scripts from attachments and generate disk.xml and its image \n[root@intel-5205-32-2 1043069]# cat disk.xml \n<disk type='file' device='disk'>\n<driver name='qemu' type='raw' cache='none'/>\n<source file='/var/lib/libvirt/images/disk1.img'/>\n<target dev='vdb' bus='virtio'/>\n</disk>\n[root@intel-5205-32-2 1043069]# ll /var/lib/libvirt/images/disk1.img\n-rw-r--r--. 1 qemu qemu 10485760 Apr 11 13:36 /var/lib/libvirt/images/disk1.img\n\n2.Execute attached python scripts in two different terminals at the same time, running around 1 hour, no crash occurred.\n\nThe first terminal:\n[root@intel-5205-32-2 1043069]# time ./attach_detach.py\nattach disk return: 0\ndetach disk return: 0\nattach disk return: 0\ndetach disk return: 0\n...(clipped)\ndetach disk return: 0\nattach disk return: 0\n^Cattach/detach disk fail\n\nreal\t77m8.458s\nuser\t0m1.613s\nsys\t0m0.497s\n\nThe second terminal:\n[root@intel-5205-32-2 1043069]# time ./domstate.py \n(41L, 167936L, 0L, 0L, -1L)\n...(clipped)\n(41L, 167936L, 0L, 0L, -1L)\nlibvirt: QEMU Driver error : invalid argument: invalid path: /var/lib/libvirt/images/disk1.img\n(0L, 0L, 0L, 0L, -1L)\n(0L, 0L, 0L, 0L, -1L)\n(0L, 0L, 0L, 0L, -1L)\n(0L, 0L, 0L, 0L, -1L)\n(0L, 0L, 0L, 0L, -1L)\n(0L, 0L, 0L, 0L, -1L)\n(0L, 0L, 0L, 0L, -1L)\n(0L, 0L, 0L, 0L, -1L)\n(0L, 0L, 0L, 0L, -1L)\n(0L, 0L, 0L, 0L, -1L)\n(0L, 0L, 0L, 0L, -1L)\n(2L, 8192L, 0L, 0L, -1L)\n(2L, 8192L, 0L, 0L, -1L)\n(2L, 8192L, 0L, 0L, -1L)\n(9L, 36864L, 0L, 0L, -1L)\n(21L, 86016L, 0L, 0L, -1L)\n(21L, 86016L, 0L, 0L, -1L)\n(21L, 86016L, 0L, 0L, -1L)\n(21L, 86016L, 0L, 0L, -1L)\n(22L, 90112L, 0L, 0L, -1L)\n(22L, 90112L, 0L, 0L, -1L)\n(22L, 90112L, 0L, 0L, -1L)\n(34L, 139264L, 0L, 0L, -1L)\n(41L, 167936L, 0L, 0L, -1L)\n(41L, 167936L, 0L, 0L, -1L)\n...(clipped)\n(41L, 167936L, 0L, 0L, -1L)\n(41L, 167936L, 0L, 0L, -1L)\n(41L, 167936L, 0L, 0L, -1L)\n^CTraceback (most recent call last):\n  File \"./domstate.py\", line 21, in <module>\n    print dom.blockStats(path)\n  File \"/usr/lib64/python2.6/site-packages/libvirt.py\", line 1837, in blockStats\n    ret = libvirtmod.virDomainBlockStats(self._o, path)\nKeyboardInterrupt\n\nreal\t77m7.032s\nuser\t1m36.901s\nsys\t0m25.276s\n\n[root@intel-5205-32-2 ~]# virsh list --all\n Id    Name                           State\n----------------------------------------------------\n 2     r6                             running\n[root@intel-5205-32-2 1043069]# service libvirtd status\nlibvirtd (pid  11167) is running...\n\nWe can get the expected results, changed to Verified.",
        "Since the problem described in this bug report should be\nresolved in a recent advisory, it has been closed with a\nresolution of ERRATA.\n\nFor information on the advisory, and where to find the updated\nfiles, follow the link below.\n\nIf the solution does not work for you, open a new bug report.http://rhn.redhat.com/errata/RHBA-2014-1374.html"
    ]
}