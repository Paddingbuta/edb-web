{
    "bugid": "1009886",
    "cveid": [
        "CVE-2013-7336"
    ],
    "summary": "CVE-2013-7336 libvirtd crashes during established spice session migration. [rhel-6.5]",
    "alias": "None",
    "product": "Red Hat Enterprise Linux 6",
    "hardware": "Unspecified",
    "os": "Unspecified",
    "url": "",
    "reported_date": "2013-09-19 12:25 UTC byMarian Krcmarik",
    "attachment": [
        "https://bugzilla-attachments.redhat.com/attachment.cgi?id=799909",
        "https://bugzilla-attachments.redhat.com/attachment.cgi?id=799920"
    ],
    "comment": [
        "Description of problem:\nSource libvirtd crashes when performing migration on 6.5 host to 6.5 remote host with established spice session (spice client connected), The host is managed by RHEVM3.3. The crash does not happen when migration VM without spice session established, I am not able to reproduce that in different setup but my original setup has more complex setup (especially in networking) It uses separated display network for Spice traffic, separated network for VMs network interfaces. That all on bonded NICs. But I could reproduce even without display network.\n\nI am attaching snip from source libvirtd where libvirtd crash is caught. As well as I attach core dump of libvirtd process.\n\nVersion-Release number of selected component (if applicable):\nrpm -qa | egrep  \"libvirt|qemu-kvm|vdsm\"\nlibvirt-client-0.10.2-24.el6.x86_64\nvdsm-xmlrpc-4.12.0-138.gitab256be.el6ev.noarch\nqemu-kvm-rhev-0.12.1.2-2.404.el6.x86_64\nlibvirt-lock-sanlock-0.10.2-24.el6.x86_64\nvdsm-cli-4.12.0-138.gitab256be.el6ev.noarch\nqemu-kvm-rhev-tools-0.12.1.2-2.404.el6.x86_64\nlibvirt-python-0.10.2-24.el6.x86_64\nvdsm-python-4.12.0-138.gitab256be.el6ev.x86_64\nvdsm-4.12.0-138.gitab256be.el6ev.x86_64\nvdsm-python-cpopen-4.12.0-138.gitab256be.el6ev.x86_64\nlibvirt-0.10.2-24.el6.x86_64\n\nHow reproducible:\nAlways on my setup.\n\nSteps to Reproduce:\n1. In RHEV3.3 environment migrate VM with established Spice session.\n\nActual results:\nSource Libvirtd crash.\n\nExpected results:\nNo crash on source.\n\nAdditional info:\nI can keep the setup for some short time.",
        "Createdattachment 799909[details]source libvirtd log\n\nSnip from libvirtd log which contains crash info.",
        "Createdattachment 799910[details]core dump of crashed source libvirtd",
        "Createdattachment 799920[details]Destination libvirtd log",
        "Patch proposed upstream:http://www.redhat.com/archives/libvir-list/2013-September/msg01208.html",
        "Hi Marian,\nI can't reproduce this error with below package in rhevm 3.2 environment.\n\nvdsm-python-4.10.2-25.0.el6ev.x86_64\nqemu-kvm-rhev-debuginfo-0.12.1.2-2.404.el6.x86_64\nlibvirt-python-0.10.2-26.el6.x86_64\nlibvirt-0.10.2-26.el6.x86_64\nlibvirt-debuginfo-0.10.2-26.el6.x86_64\nvdsm-cli-4.10.2-25.0.el6ev.noarch\nqemu-kvm-rhev-0.12.1.2-2.404.el6.x86_64\nlibvirt-lock-sanlock-0.10.2-26.el6.x86_64\nvdsm-4.10.2-25.0.el6ev.x86_64\nqemu-kvm-rhev-tools-0.12.1.2-2.404.el6.x86_64\nlibvirt-devel-0.10.2-26.el6.x86_64\nlibvirt-client-0.10.2-26.el6.x86_64\nvdsm-xmlrpc-4.10.2-25.0.el6ev.noarch\n\nDo i must setup rhevm 3.3 to repoduce this bug? AFAIK,rhevm 3.3 is not released, how can i get it?",
        "(In reply to Shanzhi Yu fromcomment #8)> Hi Marian,\n> I can't reproduce this error with below package in rhevm 3.2 environment.\n> \n> vdsm-python-4.10.2-25.0.el6ev.x86_64\n> qemu-kvm-rhev-debuginfo-0.12.1.2-2.404.el6.x86_64\n> libvirt-python-0.10.2-26.el6.x86_64\n> libvirt-0.10.2-26.el6.x86_64\n> libvirt-debuginfo-0.10.2-26.el6.x86_64\n> vdsm-cli-4.10.2-25.0.el6ev.noarch\n> qemu-kvm-rhev-0.12.1.2-2.404.el6.x86_64\n> libvirt-lock-sanlock-0.10.2-26.el6.x86_64\n> vdsm-4.10.2-25.0.el6ev.x86_64\n> qemu-kvm-rhev-tools-0.12.1.2-2.404.el6.x86_64\n> libvirt-devel-0.10.2-26.el6.x86_64\n> libvirt-client-0.10.2-26.el6.x86_64\n> vdsm-xmlrpc-4.10.2-25.0.el6ev.noarch\n> \n> Do i must setup rhevm 3.3 to repoduce this bug? AFAIK,rhevm 3.3 is not\n> released, how can i get it?Try to slown down Spice migration -> Limit bandwidth between your client machine and hosts, Install spice component (spice guest agent) on the VM and open more monitors and redirect some USB devices through the native USB redir.\n\nI am not sure, maybe 3.3 vdsm has some effect on that (http://bob.eng.lab.tlv.redhat.com/builds/is15/). \n\nI still have the setup and new build with the fix will be available soon  I assume so In the worst case I can verify myself.",
        "Colleague of mine managed to reproduse this without vdsm/rhev etc.  The point is that there must be enough domblkstat requests to get one while waiting for the spice migration to finish.  So virsh domblkstat in a cycle and slowing down the migration should be the way how to reproduce this properly.  As Martin pointed out, try slowing down the migration (through a slow network for example), make sure there is vdagent installed in the guest, and the more spice stuff is used, the more probable it is to hit this issue.",
        "(In reply to Martin Kletzander fromcomment #10)> Colleague of mine managed to reproduse this without vdsm/rhev etc.  The\n> point is that there must be enough domblkstat requests to get one while\n> waiting for the spice migration to finish.  So virsh domblkstat in a cycle\n> and slowing down the migration should be the way how to reproduce this\n> properly.  As Martin pointed out, try slowing down the migration (through a\n> slow network for example), make sure there is vdagent installed in the\n> guest, and the more spice stuff is used, the more probable it is to hit this\n> issue.Hi Martin\nI try to reproduce it without vdsm/rhev, but hardly succeed. My steps is as below, please help correct it. Thanks advance\n\n1. exist an guest installed with vdagent\n2. login guest and do some disk R/W operations in a cycle.\n3. do \"virsh domblkstat guest\" in a cycle\n4. open one spice\n5. set migrate speed to very low by \"virsh migrate-setspeed \" command  \n6. do migrate",
        "The error I met is as below:\n\nvirsh migrate --live migrate qemu+ssh://10.66.106.20/system\n2013-09-24 10:12:48.736+0000: 24726: info : libvirt version: 0.10.2, package: 24.el6 (Red Hat, Inc. <http://bugzilla.redhat.com/bugzilla>, 2013-09-06-15:56:52, x86-022.build.eng.bos.redhat.com)\n2013-09-24 10:12:48.736+0000: 24726: warning : virDomainMigrateVersion3:4922 : Guest migrate probably left in 'paused' state on source\n\nerror: One or more references were leaked after disconnect from the hypervisor",
        "(In reply to Shanzhi Yu fromcomment #12)\nDoing R/W in the guest slows down the storage part of migration and virsh migrate-setspeed accepts parameters in MiB/s.  What you need to do is slow down the spice part of the migration, which contains only a bit of data.  So what you need to do is to slow it down way more than 1MiB/s (1) and generate more data for spice to transfer (2).\n\n 1) The easiest way is to migrate over dedicated network which will be slowed down to, for example, 20KiB/s\n 2) Open more displays (monitors) and redirect some USB devices into the guest (make sure it uses spicevmc redirection)\n\nBasically everything fromcomment #9.",
        "Hi Martin,\nPlease help review my steps below, Thanks\n\nPrepare an NFS guest, mounted on source server and target server.\n1. define an guest with spicevmc redirection on source server\n\n# virsh dumpxml winxp|grep redirdev\n    <redirdev bus='usb' type='spicevmc'>\n    </redirdev>\n    <redirdev bus='usb' type='spicevmc'>\n    </redirdev>\n    <redirdev bus='usb' type='spicevmc'>\n    </redirdev>\n    <redirdev bus='usb' type='spicevmc'>\n    </redirdev>\n2. plugin two usb disk on source server.Open an diskplays(using virt-viewer) and do redirection to guest.\n\n3. configure network card speed on source server\n\n# ethtool --change eth0 autoneg off speed 10 duplex full\n# iptables -A OUTPUT -s $(source server IP) -m limit --limit 10/s -j ACCEPT\n# iptables -A OUTPUT -s $(source server IP) -j ACCEPT\n\n4. Do migration from source server to target server\n\n# time  virsh migrate --live  winxp qemu+ssh://$(Target server IP)/system\n\nBy those steps above, can't met libvirtd crash while the result is that it fail to migrate the guest. Error info can be found in libvirtd.log is \n\n2013-10-12 11:33:34.045+0000: 5406: error : virNetSocketReadWire:1184 : End of file while reading data: Input/output error\n2013-10-12 11:33:42.365+0000: 5406: error : qemuMonitorIORead:513 : Unable to read from monitor: Connection reset by peer",
        "The iptables rules you are using mean that 10 packets per second are allowed by the first rule, but the rest is allowed by the second one.  And even if you don't add the second one, it will be matched by a different one probably.\nNevertheless, the problem is that if you drop/reject non-matching packets, it will not just slow down the traffic, but you'll get a huge amount of packet loss (because all other packets will get dropped).\n\nYou need to limit the speed properly, try doing this on the destination:\n\n # tc qdisc add dev eth0 ingress\n # tc filter add dev eth0 parent ffff: protocol ip u32 match ip src 0.0.0.0/0 police rate 64kbit burst 64kbit mtu 64kb drop flowid :1\n\nBeware! This will limit all incoming data on eth0 to this machine to 64kbps.  To remove this limitation, do this:\n\n # tc qdisc del dev eth0 ingress\n\nBefore you are starting the migration, try (for example using 'nc' and 'dd' or 'pv') how fast the communication really is.  If it is more than 64kbps, the setting is not correct and you will fail reproducing the crash.",
        "Version-Release number of selected component (if applicable):\n\nqemu-kvm-rhev-0.12.1.2-2.411.el6.x86_64\nlibvirt-0.10.2-29.el6.x86_64\n\nPreparations:\n\n1. mount NFS server to both source and target server.Both source and target server has two NICs(eth0 and eth1). one is used for spice migrations which be limit low speed. define net1 on two server based on eth0\n\n2. define an guest with usbredir \n#virsh dumpxml rhel6\n\n    <redirdev bus='usb' type='spicevmc'>\n    </redirdev>\n    <redirdev bus='usb' type='spicevmc'>\n    </redirdev>\n    <redirdev bus='usb' type='spicevmc'>\n    </redirdev>\n    <redirdev bus='usb' type='spicevmc'>\n    </redirdev>\n <graphics type='spice' port='5900' autoport='no'>\n      <listen type='network' network='net1'/>\n </graphics>\n\n3. install vdagent in guest.\n# rpm -qa|grep vdagent\nspice-vdagent-0.12.0-4.el6.x86_64\n\n4. do network transport limit on source server(eth0)(limit eth0 to 64kbps ).\n# tc qdisc add dev eth0 ingress\n# tc filter add dev eth0 parent ffff: protocol ip u32 match ip src 0.0.0.0/0 police rate 64kbit burst 64kbit mtu 64kb drop flowid :1\n(has test it with \"dd\" and \"nc\", the two command above works fine)\n\nSteps:\n1. start guest \n# virsh start rhel6\n\n2. open an display by remote-view on client server use net1 and plugin two usb disk on source server. Do usb redirection.\n\n\n3. Do migration from source server to target server with eth1' IP\n# time  virsh migrate --live  rhel6 qemu+ssh://10.66.106.23/system\nroot.106.23's password: \n\nResults: succeed migrating the guest.\nSo set it to verified.",
        "Since the problem described in this bug report should be\nresolved in a recent advisory, it has been closed with a\nresolution of ERRATA.\n\nFor information on the advisory, and where to find the updated\nfiles, follow the link below.\n\nIf the solution does not work for you, open a new bug report.http://rhn.redhat.com/errata/RHBA-2013-1581.html"
    ]
}